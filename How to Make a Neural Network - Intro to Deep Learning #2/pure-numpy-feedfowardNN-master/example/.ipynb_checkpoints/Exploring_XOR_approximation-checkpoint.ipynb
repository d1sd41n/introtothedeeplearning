{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure numpy feedforward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeuralNetwork class in the FeedForwardNN file is a simple neural network class coded in python using only numpy. It implements backpropagation.\n",
    "* Optimization : Gradient Descent\n",
    "* Layers : n fully-connected layers\n",
    "* Cost function : sum squared error\n",
    "\n",
    "In this Notebook we will use this class to create a 3-layer neural network to approximate the non-linear XOR function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-0876775569fd>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-0876775569fd>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    import ../FeedforwardNN\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from progress_bar import log_progress\n",
    "import ../FeedforwardNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 9  # Number of nodes in our hidden layer\n",
    "alpha = 5  # Learning Rate\n",
    "num_epochs = 1000  # Maximum number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of a neural network\n",
    "nn = FeedforwardNN.NeuralNetwork()\n",
    "\n",
    "# Add Layers\n",
    "# Input Layer is created automatically\n",
    "nn.add_layer((2, nodes)) # Layer 2\n",
    "nn.add_layer((nodes, 1)) # Layer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the XOR function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will test the network with the XOR function :  \n",
    "\n",
    "| Input 1 | Input 2 | Output |\n",
    "| ---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "training_data = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape(4, 2, 1)\n",
    "training_labels = np.asarray([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "# The stop accuracy tells the neural network to stop training once the error rate is under the specified threshold\n",
    "# The returned iteration is the number of epochs needed to reach that accuracy\n",
    "error_rate, iteration = nn.train(training_data, training_labels, num_epochs=num_epochs, learning_rate=alpha, stop_accuracy=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the error\n",
    "error_rate = error_rate.reshape((iteration,4))\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.plot(np.arange(iteration), error_rate[:, 0], label='[0,0]')\n",
    "plt.plot(np.arange(iteration), error_rate[:, 1], label='[0,1]')\n",
    "plt.plot(np.arange(iteration), error_rate[:, 2], label='[1,0]')\n",
    "plt.plot(np.arange(iteration), error_rate[:, 3], label='[1,1]')\n",
    "plt.plot(np.arange(iteration), np.mean(error_rate, axis=1), label='mean', color='black')\n",
    "plt.title('Error')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network for XOR function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will loop through many different combinations of number of nodes and learning rate values. We can then plot the results and find the optimal pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hyperparameters\n",
    "nodes_list = np.arange(4, 10, 1)\n",
    "alpha_list = np.arange(0.1, 15, 0.1)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for all hyperparameter combinations\n",
    "num_epoch_to_train = []\n",
    "for nodes in log_progress(nodes_list, user_label='nodes'):\n",
    "    for alpha in log_progress(alpha_list, user_label='alphas', refresh=True):\n",
    "        nn = FeedforwardNN.NeuralNetwork()\n",
    "        nn.add_layer((2, nodes)) # Layer 2\n",
    "        nn.add_layer((nodes, 1)) # Layer 3\n",
    "        error_rate, iteration = nn.train(training_data, training_labels, num_epochs=num_epochs, learning_rate=alpha, stop_accuracy=1e-6)\n",
    "        num_epoch_to_train.append(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for plotting\n",
    "z = np.asarray(num_epoch_to_train).reshape(len(nodes_list), -1)\n",
    "np.savez('mesh.npz', alpha_list, nodes_list, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "for n in range(len(z)):\n",
    "    plt.plot(alpha_list, z[n], label='{n} nodes'.format(n=n+4))\n",
    "\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('epochs')\n",
    "ax.set_ylim([0,100])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_title('Epochs needed to learn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to play around with different random seeds (in the initialization of the NeuralNetwork class) when searching for the optimal pair of hyperparameters.\n",
    "For the seed above, it seems we can use a rather large alpha and only 5 nodes on the hidden layer in order to learn the fastest. However, using different seeds I found that usually using 7 or 9 nodes and a learning rate between 1 and 5 to be the best all-around solution.  \n",
    "**To be safe and garantee convergence, a small learning rate should be chosen and the number of epochs adjusted accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {
    "0b8d28024dcb4157907384322e66f143": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "6d8934f696b04f51b68a51ac48477493": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
